# model/intent/inference.py

import torch
import argparse
from safetensors.torch import load_file
from transformers import AutoTokenizer
from utils.config import load_config
from models.intent.intent_model import IntentClassifier

def infer(text: str):
    config = load_config("intent_config")
    device = torch.device("cpu")  # λλ” "cuda" μ‚¬μ© κ°€λ¥
    
    # 1) ν† ν¬λ‚μ΄μ € λ΅λ“
    tokenizer = AutoTokenizer.from_pretrained(config.model.name, use_fast=True)
    
    # 2) λ¨λΈ λ΅λ“
    model = IntentClassifier()
    model.load_state_dict(load_file(f"{config.train.output_dir}/model.safetensors"))
    model.to(device)
    model.eval()

    # 3) μ…λ ¥ ν…μ¤νΈ ν† ν¬λ‚μ΄μ§•
    inputs = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=config.max_length,
        return_tensors="pt"
    )
    
    inputs = {
        "input_ids": inputs["input_ids"].to(device),
        "attention_mask": inputs["attention_mask"].to(device)
    }

    # 4) λ¨λΈ μ¶”λ΅ 
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits.squeeze(0)
        probs = torch.softmax(logits, dim=-1)
        pred_idx = torch.argmax(probs).item()

    # 5) λΌλ²¨ λ§¤ν•‘
    label_list = config.inference.label_list
    pred_label = label_list[pred_idx] if pred_idx < len(label_list) else "(unknown)"
    
    # 6) κ²°κ³Ό μ¶λ ¥
    print(f"π“¥ μ…λ ¥: {text}")
    print(f"π― μμΈ΅λ μΈν…νΈ: {pred_label} (index: {pred_idx})")
    print(f"π“ ν™•λ¥  μƒμ„ μΈν…νΈ: {probs[pred_idx].item():.4f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="L.U.N.A. Intent Inference")
    parser.add_argument("--text", type=str, required=True, help="μ…λ ¥ ν…μ¤νΈ")
    args = parser.parse_args()

    infer(args.text)
